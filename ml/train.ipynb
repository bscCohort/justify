{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b3e7e4a",
   "metadata": {},
   "source": [
    "# BBC News Text Classification (ML Prototype)\n",
    "\n",
    "## Goal of this Notebook\n",
    "In this notebook, we will build a **text classification machine learning model**\n",
    "using the BBC News dataset.\n",
    "\n",
    "We are using this dataset to:\n",
    "- Understand how text classification works\n",
    "- Learn the end-to-end ML pipeline\n",
    "- Later integrate this model into a backend API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c2b42f",
   "metadata": {},
   "source": [
    "### What are We Building?\n",
    "\n",
    "In the BBC News Classification Project, we are building a predictive model to evaluate the various news records and classify them accordingly with the help of some parameters.The parameters into consideration are the various headlines with their respective categories. After cleaning and preprocessing the dataset using NLP techniques, we will use Machine Learning algorithms like Random Forest and SVM to classify each headline to its respective category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582905f0",
   "metadata": {},
   "source": [
    "### Pre-requisites\n",
    "\n",
    "To churn the best out of this article, the following prerequisites would be a plus:\n",
    "\n",
    "- Basic Knowledge of Python would be beneficial.\n",
    "- Implementation of libraries like Pandas, Numpy, Seaborn, Matplotlib, and SciKit Learn.\n",
    "- Understanding of Machine Learning algorithms like Random Forest, Linear, and Logistic Regression.\n",
    "- Intermediate understanding of various text cleaning and preprocessing techniques like stemming and lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e68672",
   "metadata": {},
   "source": [
    "### How Are We Going to Build This?\n",
    "\n",
    "Here's how we are going to work on this project:\n",
    "\n",
    "- Libraries - Importing the necessary NLP and ML libraries.\n",
    "- Data Analysis - This step will enable us to figure out the various values and features of the dataset.\n",
    "- Data Visualization - With basic Data Visualization, we will be able to figure out the various underlying patterns of our dataset.\n",
    "- Preprocessing - Since we are working with textual data in this project, preprocessing is very important for us to create a classifier. Using techniques like Tokenization and Lemmatization, we'll make our data model-ready.\n",
    "- Model Training - In this step, we will use various Machine Learning algorithms like Random Forest, Logistic Regression, SVC, etc., to try and create a classifier that successfully classifies news to their respective categories.\n",
    "- Model Evaluation - To make sure our classifier is working the way we want it to, we'll perform various evaluation techniques like accuracy and ROC score.\n",
    "- Model Testing - Finally, we will test our classifier with real-life data to see if it can actually predict the category of the headline.##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b24378a",
   "metadata": {},
   "source": [
    "### Final Output\n",
    "\n",
    "Our final output would be to create a classifier that can predict the headline to its respective category.\n",
    "\n",
    "```\n",
    "Headline: ['Tim Scott optimistic about Congress progress on police reform']\n",
    "POLITICS\n",
    "```\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "Environment - GitHub Codespace\n",
    "Libraries - Pandas, Numpy, Seaborn, Matplotlib, SciKit Learn, PorterStemmer, Lemmatizer, stopwords, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2743e8e2",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the Dataset\n",
    "\n",
    "We are using the **BBC News Dataset** from an official academic source:\n",
    "http://mlg.ucd.ie/datasets/bbc.html\n",
    "\n",
    "The dataset contains:\n",
    "- 2225 news articles\n",
    "- 5 categories: business, entertainment, politics, sport, tech\n",
    "\n",
    "Each article is a plain text file.\n",
    "Each folder name represents the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340dda0e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!wget http://mlg.ucd.ie/files/datasets/bbc.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0137ed50",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!unzip bbc.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bcc7a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!ls bbc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c464b2b6",
   "metadata": {},
   "source": [
    "## Step 2: Dataset Structure\n",
    "\n",
    "The dataset is organized like this:\n",
    "\n",
    "```shell\n",
    "bbc/\n",
    " ├── business/\n",
    " ├── entertainment/\n",
    " ├── politics/\n",
    " ├── sport/\n",
    " └── tech/\n",
    "```\n",
    "\n",
    "Each `.txt` file inside a folder is one document.\n",
    "The folder name is the category (label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b8536",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read Files into Python\n",
    "\n",
    "import os\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "base_path = \"bbc\"\n",
    "\n",
    "for category in os.listdir(base_path):\n",
    "    category_path = os.path.join(base_path, category)\n",
    "    for filename in os.listdir(category_path):\n",
    "        file_path = os.path.join(category_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"latin-1\") as file:\n",
    "            data.append(file.read())\n",
    "            labels.append(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1ce1e",
   "metadata": {},
   "source": [
    "## Step 3: Creating a Structured Dataset\n",
    "\n",
    "Although ML does not require CSVs, structured data\n",
    "makes experimentation easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce69142d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"text\": data,\n",
    "    \"category\": labels\n",
    "})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9a5c74",
   "metadata": {},
   "source": [
    "## Step 4: Train-Test Split\n",
    "\n",
    "We split the dataset into:\n",
    "- Training data → used to learn\n",
    "- Test data → used to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89bd77",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"text\"]\n",
    "y = df[\"category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c357b555",
   "metadata": {},
   "source": [
    "## Step 5: Text Vectorization (TF-IDF)\n",
    "\n",
    "Computers cannot understand text directly.\n",
    "We convert text into numbers using **TF-IDF**.\n",
    "\n",
    "TF-IDF measures:\n",
    "- How important a word is in a document\n",
    "- Relative to all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2e5941",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b1e276",
   "metadata": {},
   "source": [
    "## Step 6: Training the Model\n",
    "\n",
    "We use **Multinomial Naive Bayes**:\n",
    "- Simple\n",
    "- Fast\n",
    "- Works well for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732593e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0998501a",
   "metadata": {},
   "source": [
    "## Step 7: Model Evaluation\n",
    "\n",
    "We now test the model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc1bdb2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b23377",
   "metadata": {},
   "source": [
    "## Step 8: Saving the Model\n",
    "\n",
    "We save:\n",
    "- The trained model\n",
    "- The vectorizer\n",
    "\n",
    "These will be used later in the backend API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db47dd8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump((vectorizer, model), \"model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed674f38",
   "metadata": {},
   "source": [
    "## What Comes Next\n",
    "\n",
    "Next steps:\n",
    "1. Load this model in the backend\n",
    "2. Create a `/predict` API\n",
    "3. Connect frontend input to this ML model\n",
    "4. Reframe this as a legal case classifier prototype"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
